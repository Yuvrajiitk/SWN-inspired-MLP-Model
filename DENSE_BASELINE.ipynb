{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import os\n",
        "\n",
        "class MLP_CIFAR10:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 100\n",
        "        self.maxepoches = 100  # Set to your required value\n",
        "        self.learning_rate = 0.01\n",
        "        self.num_classes = 10\n",
        "        self.momentum = 0.9\n",
        "\n",
        "        self.create_model()\n",
        "        self.train()\n",
        "\n",
        "    def create_model(self):\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Flatten(input_shape=(32, 32, 3)))\n",
        "        self.model.add(Dense(4000))\n",
        "        self.model.add(BatchNormalization())                # Same as original\n",
        "        self.model.add(Activation('relu'))              # Replacing SReLU with ReLU\n",
        "        self.model.add(Dropout(0.3))\n",
        "        self.model.add(Dense(1000))\n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "        self.model.add(Dense(4000))\n",
        "        self.model.add(BatchNormalization())\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "        self.model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "    def train(self):\n",
        "        x_train, y_train, x_test, y_test = self.read_data()\n",
        "\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "        sgd = SGD(learning_rate=self.learning_rate, momentum=self.momentum)\n",
        "        self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "        history = self.model.fit(\n",
        "            datagen.flow(x_train, y_train, batch_size=self.batch_size),\n",
        "            epochs=self.maxepoches,\n",
        "            validation_data=(x_test, y_test),\n",
        "            steps_per_epoch=len(x_train) // self.batch_size,\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "        self.accuracies_per_epoch = history.history['val_accuracy']\n",
        "\n",
        "    def read_data(self):\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        y_train = to_categorical(y_train, self.num_classes)\n",
        "        y_test = to_categorical(y_test, self.num_classes)\n",
        "        x_train = x_train.astype('float32') / 255.0\n",
        "        x_test = x_test.astype('float32') / 255.0\n",
        "        return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = MLP_CIFAR10()\n",
        "\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    np.savetxt(\"results/dense_mlp_relu_sgd_cifar10_acc.txt\", np.asarray(model.accuracies_per_epoch))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zy_pZt2mdVaz",
        "outputId": "d3b40176-18e7-41fc-dabd-7ffa55c48aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │    \u001b[38;5;34m12,292,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │     \u001b[38;5;34m4,001,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │     \u001b[38;5;34m4,004,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m40,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,292,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,001,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,004,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,337,010\u001b[0m (77.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,337,010</span> (77.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,337,010\u001b[0m (77.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,337,010</span> (77.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500/500 - 32s - 64ms/step - accuracy: 0.2861 - loss: 1.9593 - val_accuracy: 0.3734 - val_loss: 1.7593\n",
            "Epoch 2/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.3451 - loss: 1.8165 - val_accuracy: 0.3878 - val_loss: 1.7002\n",
            "Epoch 3/100\n",
            "500/500 - 42s - 83ms/step - accuracy: 0.3658 - loss: 1.7675 - val_accuracy: 0.4180 - val_loss: 1.6308\n",
            "Epoch 4/100\n",
            "500/500 - 26s - 51ms/step - accuracy: 0.3779 - loss: 1.7388 - val_accuracy: 0.4320 - val_loss: 1.5822\n",
            "Epoch 5/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.3867 - loss: 1.7090 - val_accuracy: 0.4166 - val_loss: 1.6128\n",
            "Epoch 6/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.3913 - loss: 1.6910 - val_accuracy: 0.4520 - val_loss: 1.5389\n",
            "Epoch 7/100\n",
            "500/500 - 41s - 81ms/step - accuracy: 0.3989 - loss: 1.6662 - val_accuracy: 0.4575 - val_loss: 1.5303\n",
            "Epoch 8/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.4070 - loss: 1.6431 - val_accuracy: 0.4430 - val_loss: 1.5440\n",
            "Epoch 9/100\n",
            "500/500 - 27s - 55ms/step - accuracy: 0.4151 - loss: 1.6315 - val_accuracy: 0.4738 - val_loss: 1.5023\n",
            "Epoch 10/100\n",
            "500/500 - 27s - 54ms/step - accuracy: 0.4182 - loss: 1.6166 - val_accuracy: 0.4732 - val_loss: 1.4864\n",
            "Epoch 11/100\n",
            "500/500 - 40s - 81ms/step - accuracy: 0.4259 - loss: 1.5989 - val_accuracy: 0.4802 - val_loss: 1.4592\n",
            "Epoch 12/100\n",
            "500/500 - 42s - 83ms/step - accuracy: 0.4281 - loss: 1.5877 - val_accuracy: 0.4690 - val_loss: 1.4834\n",
            "Epoch 13/100\n",
            "500/500 - 40s - 81ms/step - accuracy: 0.4319 - loss: 1.5785 - val_accuracy: 0.4925 - val_loss: 1.4255\n",
            "Epoch 14/100\n",
            "500/500 - 26s - 53ms/step - accuracy: 0.4385 - loss: 1.5568 - val_accuracy: 0.4896 - val_loss: 1.4479\n",
            "Epoch 15/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.4409 - loss: 1.5473 - val_accuracy: 0.4743 - val_loss: 1.4502\n",
            "Epoch 16/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.4463 - loss: 1.5405 - val_accuracy: 0.5050 - val_loss: 1.4201\n",
            "Epoch 17/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.4476 - loss: 1.5302 - val_accuracy: 0.5001 - val_loss: 1.4019\n",
            "Epoch 18/100\n",
            "500/500 - 26s - 51ms/step - accuracy: 0.4555 - loss: 1.5166 - val_accuracy: 0.4946 - val_loss: 1.4171\n",
            "Epoch 19/100\n",
            "500/500 - 42s - 83ms/step - accuracy: 0.4532 - loss: 1.5121 - val_accuracy: 0.5122 - val_loss: 1.3787\n",
            "Epoch 20/100\n",
            "500/500 - 41s - 81ms/step - accuracy: 0.4587 - loss: 1.5019 - val_accuracy: 0.5092 - val_loss: 1.3792\n",
            "Epoch 21/100\n",
            "500/500 - 26s - 53ms/step - accuracy: 0.4645 - loss: 1.4918 - val_accuracy: 0.5087 - val_loss: 1.3805\n",
            "Epoch 22/100\n",
            "500/500 - 27s - 54ms/step - accuracy: 0.4613 - loss: 1.4867 - val_accuracy: 0.5175 - val_loss: 1.3552\n",
            "Epoch 23/100\n",
            "500/500 - 26s - 53ms/step - accuracy: 0.4664 - loss: 1.4807 - val_accuracy: 0.5039 - val_loss: 1.3795\n",
            "Epoch 24/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.4713 - loss: 1.4710 - val_accuracy: 0.5281 - val_loss: 1.3583\n",
            "Epoch 25/100\n",
            "500/500 - 42s - 84ms/step - accuracy: 0.4742 - loss: 1.4604 - val_accuracy: 0.5236 - val_loss: 1.3396\n",
            "Epoch 26/100\n",
            "500/500 - 40s - 80ms/step - accuracy: 0.4753 - loss: 1.4559 - val_accuracy: 0.5250 - val_loss: 1.3458\n",
            "Epoch 27/100\n",
            "500/500 - 26s - 53ms/step - accuracy: 0.4784 - loss: 1.4504 - val_accuracy: 0.5262 - val_loss: 1.3402\n",
            "Epoch 28/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.4775 - loss: 1.4464 - val_accuracy: 0.5208 - val_loss: 1.3580\n",
            "Epoch 29/100\n",
            "500/500 - 42s - 84ms/step - accuracy: 0.4859 - loss: 1.4363 - val_accuracy: 0.5231 - val_loss: 1.3352\n",
            "Epoch 30/100\n",
            "500/500 - 40s - 79ms/step - accuracy: 0.4822 - loss: 1.4345 - val_accuracy: 0.5301 - val_loss: 1.3121\n",
            "Epoch 31/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.4866 - loss: 1.4212 - val_accuracy: 0.5378 - val_loss: 1.2989\n",
            "Epoch 32/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.4898 - loss: 1.4146 - val_accuracy: 0.5333 - val_loss: 1.3078\n",
            "Epoch 33/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.4934 - loss: 1.4093 - val_accuracy: 0.5374 - val_loss: 1.3026\n",
            "Epoch 34/100\n",
            "500/500 - 27s - 53ms/step - accuracy: 0.4935 - loss: 1.4113 - val_accuracy: 0.5353 - val_loss: 1.3073\n",
            "Epoch 35/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.4949 - loss: 1.4004 - val_accuracy: 0.5346 - val_loss: 1.2904\n",
            "Epoch 36/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.4974 - loss: 1.3964 - val_accuracy: 0.5411 - val_loss: 1.2949\n",
            "Epoch 37/100\n",
            "500/500 - 40s - 80ms/step - accuracy: 0.5018 - loss: 1.3898 - val_accuracy: 0.5451 - val_loss: 1.2906\n",
            "Epoch 38/100\n",
            "500/500 - 25s - 51ms/step - accuracy: 0.5014 - loss: 1.3845 - val_accuracy: 0.5448 - val_loss: 1.2866\n",
            "Epoch 39/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5035 - loss: 1.3825 - val_accuracy: 0.5474 - val_loss: 1.2687\n",
            "Epoch 40/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.5092 - loss: 1.3736 - val_accuracy: 0.5464 - val_loss: 1.2719\n",
            "Epoch 41/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.5100 - loss: 1.3685 - val_accuracy: 0.5461 - val_loss: 1.2667\n",
            "Epoch 42/100\n",
            "500/500 - 40s - 80ms/step - accuracy: 0.5068 - loss: 1.3720 - val_accuracy: 0.5560 - val_loss: 1.2560\n",
            "Epoch 43/100\n",
            "500/500 - 27s - 54ms/step - accuracy: 0.5104 - loss: 1.3634 - val_accuracy: 0.5596 - val_loss: 1.2473\n",
            "Epoch 44/100\n",
            "500/500 - 40s - 80ms/step - accuracy: 0.5094 - loss: 1.3612 - val_accuracy: 0.5515 - val_loss: 1.2546\n",
            "Epoch 45/100\n",
            "500/500 - 43s - 86ms/step - accuracy: 0.5146 - loss: 1.3529 - val_accuracy: 0.5500 - val_loss: 1.2744\n",
            "Epoch 46/100\n",
            "500/500 - 39s - 77ms/step - accuracy: 0.5123 - loss: 1.3556 - val_accuracy: 0.5527 - val_loss: 1.2677\n",
            "Epoch 47/100\n",
            "500/500 - 26s - 53ms/step - accuracy: 0.5134 - loss: 1.3480 - val_accuracy: 0.5645 - val_loss: 1.2385\n",
            "Epoch 48/100\n",
            "500/500 - 26s - 53ms/step - accuracy: 0.5182 - loss: 1.3404 - val_accuracy: 0.5565 - val_loss: 1.2538\n",
            "Epoch 49/100\n",
            "500/500 - 41s - 81ms/step - accuracy: 0.5172 - loss: 1.3357 - val_accuracy: 0.5613 - val_loss: 1.2405\n",
            "Epoch 50/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.5200 - loss: 1.3349 - val_accuracy: 0.5597 - val_loss: 1.2335\n",
            "Epoch 51/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5207 - loss: 1.3333 - val_accuracy: 0.5578 - val_loss: 1.2481\n",
            "Epoch 52/100\n",
            "500/500 - 40s - 80ms/step - accuracy: 0.5239 - loss: 1.3301 - val_accuracy: 0.5622 - val_loss: 1.2389\n",
            "Epoch 53/100\n",
            "500/500 - 41s - 81ms/step - accuracy: 0.5261 - loss: 1.3196 - val_accuracy: 0.5588 - val_loss: 1.2496\n",
            "Epoch 54/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.5215 - loss: 1.3227 - val_accuracy: 0.5653 - val_loss: 1.2374\n",
            "Epoch 55/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5274 - loss: 1.3156 - val_accuracy: 0.5738 - val_loss: 1.2108\n",
            "Epoch 56/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5271 - loss: 1.3153 - val_accuracy: 0.5677 - val_loss: 1.2248\n",
            "Epoch 57/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5272 - loss: 1.3130 - val_accuracy: 0.5659 - val_loss: 1.2205\n",
            "Epoch 58/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5297 - loss: 1.3071 - val_accuracy: 0.5637 - val_loss: 1.2329\n",
            "Epoch 59/100\n",
            "500/500 - 40s - 81ms/step - accuracy: 0.5292 - loss: 1.3063 - val_accuracy: 0.5700 - val_loss: 1.2087\n",
            "Epoch 60/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.5354 - loss: 1.2971 - val_accuracy: 0.5675 - val_loss: 1.2117\n",
            "Epoch 61/100\n",
            "500/500 - 26s - 51ms/step - accuracy: 0.5360 - loss: 1.2946 - val_accuracy: 0.5681 - val_loss: 1.2173\n",
            "Epoch 62/100\n",
            "500/500 - 26s - 51ms/step - accuracy: 0.5356 - loss: 1.2936 - val_accuracy: 0.5688 - val_loss: 1.2027\n",
            "Epoch 63/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.5366 - loss: 1.2894 - val_accuracy: 0.5691 - val_loss: 1.2123\n",
            "Epoch 64/100\n",
            "500/500 - 40s - 81ms/step - accuracy: 0.5390 - loss: 1.2802 - val_accuracy: 0.5696 - val_loss: 1.2117\n",
            "Epoch 65/100\n",
            "500/500 - 26s - 51ms/step - accuracy: 0.5404 - loss: 1.2771 - val_accuracy: 0.5715 - val_loss: 1.1899\n",
            "Epoch 66/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5365 - loss: 1.2797 - val_accuracy: 0.5761 - val_loss: 1.1914\n",
            "Epoch 67/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5421 - loss: 1.2785 - val_accuracy: 0.5734 - val_loss: 1.2102\n",
            "Epoch 68/100\n",
            "500/500 - 41s - 81ms/step - accuracy: 0.5410 - loss: 1.2737 - val_accuracy: 0.5763 - val_loss: 1.2015\n",
            "Epoch 69/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5435 - loss: 1.2671 - val_accuracy: 0.5748 - val_loss: 1.1897\n",
            "Epoch 70/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5471 - loss: 1.2646 - val_accuracy: 0.5793 - val_loss: 1.1752\n",
            "Epoch 71/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.5424 - loss: 1.2647 - val_accuracy: 0.5864 - val_loss: 1.1746\n",
            "Epoch 72/100\n",
            "500/500 - 27s - 53ms/step - accuracy: 0.5449 - loss: 1.2605 - val_accuracy: 0.5798 - val_loss: 1.1803\n",
            "Epoch 73/100\n",
            "500/500 - 27s - 54ms/step - accuracy: 0.5448 - loss: 1.2607 - val_accuracy: 0.5719 - val_loss: 1.2024\n",
            "Epoch 74/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5504 - loss: 1.2517 - val_accuracy: 0.5754 - val_loss: 1.1832\n",
            "Epoch 75/100\n",
            "500/500 - 41s - 81ms/step - accuracy: 0.5485 - loss: 1.2516 - val_accuracy: 0.5804 - val_loss: 1.1775\n",
            "Epoch 76/100\n",
            "500/500 - 26s - 53ms/step - accuracy: 0.5537 - loss: 1.2443 - val_accuracy: 0.5822 - val_loss: 1.1882\n",
            "Epoch 77/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5525 - loss: 1.2430 - val_accuracy: 0.5885 - val_loss: 1.1754\n",
            "Epoch 78/100\n",
            "500/500 - 42s - 83ms/step - accuracy: 0.5561 - loss: 1.2393 - val_accuracy: 0.5747 - val_loss: 1.1945\n",
            "Epoch 79/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5529 - loss: 1.2391 - val_accuracy: 0.5817 - val_loss: 1.1808\n",
            "Epoch 80/100\n",
            "500/500 - 40s - 81ms/step - accuracy: 0.5550 - loss: 1.2407 - val_accuracy: 0.5823 - val_loss: 1.1922\n",
            "Epoch 81/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.5555 - loss: 1.2350 - val_accuracy: 0.5909 - val_loss: 1.1666\n",
            "Epoch 82/100\n",
            "500/500 - 26s - 53ms/step - accuracy: 0.5588 - loss: 1.2289 - val_accuracy: 0.5921 - val_loss: 1.1702\n",
            "Epoch 83/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.5560 - loss: 1.2272 - val_accuracy: 0.5853 - val_loss: 1.1838\n",
            "Epoch 84/100\n",
            "500/500 - 40s - 81ms/step - accuracy: 0.5583 - loss: 1.2263 - val_accuracy: 0.5819 - val_loss: 1.1667\n",
            "Epoch 85/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5602 - loss: 1.2201 - val_accuracy: 0.5845 - val_loss: 1.1741\n",
            "Epoch 86/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.5633 - loss: 1.2119 - val_accuracy: 0.5883 - val_loss: 1.1659\n",
            "Epoch 87/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5582 - loss: 1.2185 - val_accuracy: 0.5886 - val_loss: 1.1591\n",
            "Epoch 88/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.5652 - loss: 1.2115 - val_accuracy: 0.5947 - val_loss: 1.1555\n",
            "Epoch 89/100\n",
            "500/500 - 41s - 81ms/step - accuracy: 0.5633 - loss: 1.2150 - val_accuracy: 0.5850 - val_loss: 1.1704\n",
            "Epoch 90/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5688 - loss: 1.2020 - val_accuracy: 0.5905 - val_loss: 1.1642\n",
            "Epoch 91/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5665 - loss: 1.2024 - val_accuracy: 0.5922 - val_loss: 1.1550\n",
            "Epoch 92/100\n",
            "500/500 - 41s - 81ms/step - accuracy: 0.5650 - loss: 1.2045 - val_accuracy: 0.5930 - val_loss: 1.1549\n",
            "Epoch 93/100\n",
            "500/500 - 41s - 83ms/step - accuracy: 0.5686 - loss: 1.1978 - val_accuracy: 0.5866 - val_loss: 1.1687\n",
            "Epoch 94/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5687 - loss: 1.2021 - val_accuracy: 0.5925 - val_loss: 1.1507\n",
            "Epoch 95/100\n",
            "500/500 - 42s - 85ms/step - accuracy: 0.5703 - loss: 1.1948 - val_accuracy: 0.5900 - val_loss: 1.1626\n",
            "Epoch 96/100\n",
            "500/500 - 39s - 78ms/step - accuracy: 0.5714 - loss: 1.1944 - val_accuracy: 0.5947 - val_loss: 1.1395\n",
            "Epoch 97/100\n",
            "500/500 - 42s - 83ms/step - accuracy: 0.5721 - loss: 1.1907 - val_accuracy: 0.5910 - val_loss: 1.1606\n",
            "Epoch 98/100\n",
            "500/500 - 26s - 51ms/step - accuracy: 0.5732 - loss: 1.1862 - val_accuracy: 0.5927 - val_loss: 1.1513\n",
            "Epoch 99/100\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.5734 - loss: 1.1849 - val_accuracy: 0.5925 - val_loss: 1.1551\n",
            "Epoch 100/100\n",
            "500/500 - 26s - 52ms/step - accuracy: 0.5738 - loss: 1.1806 - val_accuracy: 0.5979 - val_loss: 1.1416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "\n",
        "class MLP_CIFAR10:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 100\n",
        "        self.maxepoches = 100\n",
        "        self.initial_epochs = 5\n",
        "        self.learning_rate = 0.01\n",
        "        self.num_classes = 10\n",
        "        self.momentum = 0.9\n",
        "        self.prune_percent = 0.2  # 20% pruning\n",
        "\n",
        "        self.create_model()\n",
        "        self.train()\n",
        "\n",
        "    def create_model(self):\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Flatten(input_shape=(32, 32, 3)))\n",
        "        self.model.add(Dense(4000))                     # Layer 1\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "        self.model.add(Dense(1000))                     # Layer 2\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "        self.model.add(Dense(4000))                     # Layer 3\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Dropout(0.3))\n",
        "        self.model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "    def train(self):\n",
        "        x_train, y_train, x_test, y_test = self.read_data()\n",
        "\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        self.model.summary()\n",
        "        sgd = SGD(learning_rate=self.learning_rate, momentum=self.momentum)\n",
        "        self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "        # Train initially\n",
        "        self.model.fit(\n",
        "            datagen.flow(x_train, y_train, batch_size=self.batch_size),\n",
        "            epochs=self.initial_epochs,\n",
        "            validation_data=(x_test, y_test),\n",
        "            steps_per_epoch=len(x_train) // self.batch_size,\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "        # Prune + Rewire\n",
        "        total_before = self.count_total_nonzero_weights()\n",
        "        self.prune_weights(self.prune_percent)\n",
        "        total_after_prune = self.count_total_nonzero_weights()\n",
        "        rewiring_needed = total_before - total_after_prune\n",
        "        self.rewire_model_balanced(rewiring_needed)\n",
        "        total_after_rewire = self.count_total_nonzero_weights()\n",
        "\n",
        "        print(f\"[INFO] Non-zero weights: before={total_before}, after prune={total_after_prune}, after rewire={total_after_rewire}\")\n",
        "\n",
        "        # Continue training\n",
        "        history = self.model.fit(\n",
        "            datagen.flow(x_train, y_train, batch_size=self.batch_size),\n",
        "            epochs=self.maxepoches,\n",
        "            initial_epoch=self.initial_epochs,\n",
        "            validation_data=(x_test, y_test),\n",
        "            steps_per_epoch=len(x_train) // self.batch_size,\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "        self.accuracies_per_epoch = history.history['val_accuracy']\n",
        "\n",
        "    def read_data(self):\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        y_train = to_categorical(y_train, self.num_classes)\n",
        "        y_test = to_categorical(y_test, self.num_classes)\n",
        "        x_train = x_train.astype('float32') / 255.0\n",
        "        x_test = x_test.astype('float32') / 255.0\n",
        "        return x_train, y_train, x_test, y_test\n",
        "\n",
        "    def count_total_nonzero_weights(self):\n",
        "        total = 0\n",
        "        for layer in self.model.layers:\n",
        "            if isinstance(layer, Dense):\n",
        "                W, _ = layer.get_weights()\n",
        "                total += np.count_nonzero(W)\n",
        "        return total\n",
        "\n",
        "    def prune_weights(self, prune_percent=0.2):\n",
        "        for layer in self.model.layers:\n",
        "            if isinstance(layer, Dense):\n",
        "                weights, biases = layer.get_weights()\n",
        "                flat = np.abs(weights.flatten())\n",
        "                threshold = np.percentile(flat, prune_percent * 100)\n",
        "                weights[np.abs(weights) < threshold] = 0.0\n",
        "                layer.set_weights([weights, biases])\n",
        "\n",
        "    def rewire_model_balanced(self, rewiring_needed):\n",
        "        print(f\"\\n[INFO] Rewiring {rewiring_needed} connections...\")\n",
        "\n",
        "        dense_layers = [layer for layer in self.model.layers if isinstance(layer, Dense)]\n",
        "        layer_weights = [layer.get_weights() for layer in dense_layers]\n",
        "\n",
        "        valid_layer_pairs = [(i, j) for i in range(len(dense_layers))\n",
        "                             for j in range(len(dense_layers)) if abs(i - j) > 1]\n",
        "\n",
        "        connections_added = 0\n",
        "        attempt = 0\n",
        "        max_attempts = rewiring_needed * 5\n",
        "\n",
        "        while connections_added < rewiring_needed and attempt < max_attempts:\n",
        "            src_idx, dst_idx = valid_layer_pairs[np.random.randint(0, len(valid_layer_pairs))]\n",
        "            W_src, _ = layer_weights[src_idx]\n",
        "            W_dst, b_dst = layer_weights[dst_idx]\n",
        "\n",
        "            src_neurons = np.where(np.any(W_src != 0, axis=0))[0]\n",
        "            dst_inputs = np.where(np.all(W_dst == 0, axis=1))[0]\n",
        "\n",
        "            if len(src_neurons) == 0 or len(dst_inputs) == 0:\n",
        "                attempt += 1\n",
        "                continue\n",
        "\n",
        "            src = np.random.choice(src_neurons)\n",
        "            dst = np.random.choice(dst_inputs)\n",
        "            if W_dst[dst, src % W_dst.shape[1]] == 0:\n",
        "                W_dst[dst, src % W_dst.shape[1]] = np.random.normal(0, 0.05)\n",
        "                layer_weights[dst_idx][0] = W_dst\n",
        "                connections_added += 1\n",
        "            attempt += 1\n",
        "\n",
        "        for i, layer in enumerate(dense_layers):\n",
        "            layer.set_weights(layer_weights[i])\n",
        "\n",
        "        print(f\"[INFO] Rewiring complete. Connections added: {connections_added}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = MLP_CIFAR10()\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    np.savetxt(\"results/dense_mlp_balanced_pruned_rewired_acc.txt\", np.asarray(model.accuracies_per_epoch))\n"
      ],
      "metadata": {
        "id": "6sScS-mndEuB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "outputId": "f30c5ea2-f85c-46c4-d90c-d4009ece0bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │    \u001b[38;5;34m12,292,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │     \u001b[38;5;34m4,001,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │     \u001b[38;5;34m4,004,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m40,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,292,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,001,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,004,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,337,010\u001b[0m (77.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,337,010</span> (77.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,337,010\u001b[0m (77.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,337,010</span> (77.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500/500 - 39s - 78ms/step - accuracy: 0.2796 - loss: 1.9689 - val_accuracy: 0.3596 - val_loss: 1.7801\n",
            "Epoch 2/5\n",
            "500/500 - 32s - 63ms/step - accuracy: 0.3427 - loss: 1.8204 - val_accuracy: 0.4113 - val_loss: 1.6585\n",
            "Epoch 3/5\n",
            "500/500 - 30s - 61ms/step - accuracy: 0.3624 - loss: 1.7738 - val_accuracy: 0.4234 - val_loss: 1.6137\n",
            "Epoch 4/5\n",
            "500/500 - 41s - 82ms/step - accuracy: 0.3727 - loss: 1.7399 - val_accuracy: 0.4233 - val_loss: 1.5999\n",
            "Epoch 5/5\n",
            "500/500 - 31s - 62ms/step - accuracy: 0.3862 - loss: 1.7115 - val_accuracy: 0.4277 - val_loss: 1.5883\n",
            "\n",
            "[INFO] Rewiring 4065600 connections...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6X9LCHCKX4m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}